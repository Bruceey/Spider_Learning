



# 协程的使用



**是一种用户态的轻量级线程，本质上是单线程。**

**协程允许一个执行过程A中断，然后转到执行过程B，在适当的时候再一次转回来，有点类似于多线程**。

**协程的数量理论上可以是无限个，而且没有线程之间的切换动作，执行效率比线程高。**

协程在Python中用到的是自带的标准库**asyncio**

以下介绍协程中的常用术语：



## 1、event_loop(事件循环)

事件循环，相当于一个无限循环

后面讲的协程函数都要注册到这个循环上才能运行

当满足条件发生的时候（如前一个协程对象阻塞挂起时），就会调用对应的处理方法

Python创建事件循环，示例1:

```
loop = asyncio.get_event_loop()
```



## 2、async和await

这两个关键字常用来修饰函数，被修饰的函数称为协程函数

协程函数的调用方式和普通函数相同，只不过调用协程函数并不会立即执行函数体内的内容，会生成一个coroutine协程对象(解释见下方)

这个协程对象需要放入event-loop事件循环中调度执行

Python代码示例2：

```python
async def func(url):
    print(url)
    await asyncio.sleep(1)
```

async放在函数定义的开头；await放在函数内部，用在需要挂起等待的位置，告诉cpu执行到该位置时要切换到其他任务执行。

## 3、Coroutine(协程)

在Python中指代协程对象

需要将协程对象需要放入event-loop事件循环中调度执行

Python代码示例3：

```python
import asyncio

async def func(url):
    print(url)
    await asyncio.sleep(1)
loop = asyncio.get_event_loop()    
loop.run_until_complete(func('http://www.netbian.com/meinv/'))
```



## 4、Task

Task任务对协程进一步封装，其中包含任务的各种状态。（比如是否是挂起状态）
协程对象不能直接运行，**在注册事件循环的时候，其实是run_until_complete方法将协程包装成为了一个任务（task）对象。**



## 5、Future

Future代表将来执行或没有执行的任务的结果。它和task上没有本质的区别
通常不需要在应用程序级别代码中创建Future对象。

Python代码示例5：

```python
import asyncio

urls = [f"http://www.netbian.com/meinv/index_{page}.htm" for page in range(1, 178)]


async def func(url):
    print(url)
    await asyncio.sleep(1)


async def main():
    tasks = [asyncio.ensure_future(func(url)) for url in urls]
    results = await asyncio.gather(*tasks)


if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
```

**注意：**

这段代码是我们最终在python里面常写的代码，我们定义一个main函数，函数内部用asyncio.ensure_future来创建Future对象列表，利用asyncio.gather函数来并发运行这些任务。

asyncio.gather还会返回协程函数的返回值，顺序和我们加入的任务顺序相同。

最后用loop.run_until_complete(main())调用这个主函数即可。



# aiohttp

**aiohttp是asyncio和Python的异步HTTP客户端/服务器。**

**爬虫常用的是HTTP客户端，跟requests库一样为网络请求库。**

**requests库是同步的，不能结合asyncio使用，aiohttp常和asyncio库结合使用。**

基本使用如下：

```python
import asyncio
import aiohttp


async def main():
    data = {'name': 'germeey', 'age': 25}
    # 创建ClientSession客户端，之后就可以发送请求了
    async with aiohttp.ClientSession() as session:
        async with session.get('https://httpbin.org/get') as response:
            print('status', response.status)
            print('body', await response.text())
            print('bytes', await response.read())
            print('json', await response.json())


if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
    # asyncio.run(main())  3.7之后
```



## 1、ClientSession()

我们每次发送请求时，会首先创建一个客户端（类似于浏览器），利用客户端来发送请求。

我们可以以下方式创建：

```python
async with aiohttp.ClientSession() as session：
```

这种方式创建就不用手动关闭。另一种是手动关闭的形式：

```python
session = aiohttp.ClientSession()
await session.close()
```

这种方式较为常用。因为我们一般会把session作为全局变量使用，这样就不必每个网络请求函数都传入这个session参数。后续代码会见到。

## 2、请求方式

请求方式的用法和requests库差不多。

get请求：

```
session.get(url)
```

post请求：

```
session.post(url, data=xxx)
```



以下讲解ClientSession()中常常定义的参数

## 3、headers参数

为每个请求方式添加请求头，这样我们就不必在get或post请求中再加入请求头了

```python
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36',
}
session = aiohttp.ClientSession(headers=headers)
```



## 4、timeout超时设置

防止程序在某个网络请求上一直阻塞，需要定义这个请求参数。默认设置是5分钟

我们需要先定义一个aiohttp.ClientTimeout(total)对象，之后传给timeout参数。

```python
timeout = aiohttp.ClientTimeout(3)
session = aiohttp.ClientSession(timeout=timeout, headers=headers)
```

如果超时会报错，这时我们在相应位置进行捕获处理。



## 5、ssl证书设置

如果爬取没有ssl证书的网站就会报ssl证书的错误。我们需要以下设置：

```
session = aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False))
```



## 6、获取响应

我们利用response = session.get(url)得到响应后，通过以下方式获取具体内容：

```python
# 获取响应状态码
status = response.status
# 获取html代码
html = await response.text()
# 获取二进制内容
bytes = await response.read()
# 获取json格式的响应内容, 返回的是字典
json = await response.json()
```

**注意：**

response.text()通常会猜测出charset编码，如果不准确需要自己指定。在 **text(encoding='编码')** 即可。



## 7、控制异步爬取的并发量

通常协程的数量是可以无限多的，但考虑到网站并发量的承受程度，如果并发量过大，可能导致网站挂掉。

我们可以通过控制并发量来解决这个问题，利用asyncio.Semaphore(CONCURRENCY)即可，括号里面的参数即自定义的并发量。

之后将此代码嵌套在get或post请求的外面

```python
async def scrape_api(url):
  async with asyncio.Semaphore(CONCURRENCY):
    async with session.get(url) as response:
      return await response.json()
```



## 8、异步爬取美女壁纸案例

```python
# 抓取多页
import time
from bs4 import BeautifulSoup
import os
import atexit
import asyncio
import aiohttp
# 日志模块
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36',
}
# 并发量
CONCURRENCY = 20
INDEX_URL = 'http://www.netbian.com/meinv/index_{page}.htm'
# 下载的总页数
PAGE_NUMBER = 177
# 在此声明这个变量类型，方便pycharm提示它的相应方法
session: aiohttp.ClientSession


# 程序停止会自动计算下载总时间和下载速度
@atexit.register
def calc_time():
    """
    日志
    :return:
    """
    # 计算下载总时间
    end = time.time()
    duration = end - start
    hour = int(duration / 3600)
    minute = int((duration - hour * 3600) / 60)
    seconds = int(duration - hour * 3600 - minute * 60)

    # 计算下载速度
    size = 0  # 单位是字节
    files = os.listdir("image")
    for file in files:
        try:
            size += os.path.getsize("./image/" + file)
        except Exception as e:
            print(e)
    # 单位是M
    size = size / 1024 / 1024
    # 单位是kb/s
    speed = size * 1024 / duration

    print("\033[31m=" * 100)
    print("一共下载了{}个文件, 大小为{:.2f}M".format(len(files), size))
    print("下载速度为{:.2f} kb/s".format(speed))
    print("耗时{}小时{}分钟{}秒".format(hour, minute, seconds))
    print("=" * 100)


# 请求url，得到html代码
async def get_url(url):
    try:
        async with asyncio.Semaphore(CONCURRENCY):
            logging.info('scraping %s', url)
            async with session.get(url) as response:
                return await response.text('gbk')
    except Exception as e:
        logging.error('error occurred while scraping %s\nthe reason is %s', url, e, exc_info=True)


# 请求url，得到响应的二进制文件
async def get_bytes(url):
    try:
        async with asyncio.Semaphore(CONCURRENCY):
            logging.info('scraping %s', url)
            async with session.get(url) as response:
                return await response.read()
    except Exception as e:
        logging.error('error occurred while scraping %s\nthe reason is %s', url, e, exc_info=True)


# 下载图片
async def download(url):
    print("正在下载链接地址为 %s 的图片" % url)
    # 二进制文件
    content = await get_bytes(url)
    # url的格式 http://img.netbian.com/file/2021/0122/2861bb5516bd41b0dfe79f6a9538892d.jpg
    # 取最后一个"/"之后的字符串作为文件名
    filename = url.split("/")[-1]
    # 拼写完整的图片路径，其中这里的"."表示当前这个文件所在的目录
    file_path = "./image/" + filename
    # 将二进制数据写入文件
    with open(file_path, 'wb') as f:
        f.write(content)


async def run(url):
    html = await get_url(url)
    # 利用BeautifulSoup构建解析器
    soup = BeautifulSoup(html, "lxml")

    # 选取所有的图片所在的块区域
    aElements = soup.select('.list a')
    hrefs = [i["href"] for i in aElements]

    for href in hrefs:
        if href.startswith("/desk"):
            # url2是缩略图对应的链接
            url2 = "http://www.netbian.com" + href

            # 请求缩略图链接得到页面内容
            html2 = await get_url(url2)
            soup2 = BeautifulSoup(html2, "lxml")
            src = soup2.select('.pic img')[0]["src"]
            await download(src)


async def main():
    global session
    # 判断当前目录下是否有image文件夹，没有就创建
    if not os.path.exists("image"):
        os.mkdir("image")

    session = aiohttp.ClientSession(headers=headers)
    scrape_index_tasks = [asyncio.ensure_future(run(INDEX_URL.format(page=page))) for page in range(1, PAGE_NUMBER + 1)]
    scrape_index_tasks[0] = asyncio.ensure_future(run('http://www.netbian.com/meinv/'))
    results = await asyncio.gather(*scrape_index_tasks)
    await session.close()


if __name__ == '__main__':
    start = time.time()
    # asyncio.get_event_loop().run_until_complete(main())
    asyncio.run(main())
```